# ğŸš— Oficina

AplicaÃ§Ã£o para simular o fluxo de execuÃ§Ã£o de serviÃ§os em uma oficina mecÃ¢nica, cobrindo desde o orÃ§amento atÃ© a conclusÃ£o da ordem de serviÃ§o.

---

## ğŸ“‹ DescriÃ§Ã£o

O **Oficina** Ã© um sistema monolÃ­tico construÃ­do em **Java 21** com **Spring Boot** e **PostgreSQL**. A sua arquitetura Ã© baseada no padrÃ£o **Ports and Adapters (Arquitetura Hexagonal)**, que isola a lÃ³gica de negÃ³cio de detalhes de infraestrutura.

O sistema foi projetado para simular processos tÃ­picos de uma oficina, como:

-   Registro e gerenciamento de clientes
-   CriaÃ§Ã£o e aprovaÃ§Ã£o de orÃ§amentos
-   EmissÃ£o e acompanhamento de ordens de serviÃ§o
-   Controle de peÃ§as, insumos e serviÃ§os executados
-   SeguranÃ§a e controle de acesso

A aplicaÃ§Ã£o foi desenvolvida com foco em **boas prÃ¡ticas**, **DDD**, e uma arquitetura limpa para garantir **separaÃ§Ã£o de responsabilidades**, **testabilidade** e **manutenibilidade**.

---

## ğŸ› ï¸ Tecnologias Utilizadas

-   **Java 21**
-   **Spring Boot**
-   **PostgreSQL**
-   **Gradle** (gerenciamento de dependÃªncias)
-   **JUnit 5** (testes automatizados)
-   **Terraform**
-   **AWS** (deploy na cloud)


---

## DocumentaÃ§Ã£o da API

A documentaÃ§Ã£o da API estÃ¡ disponÃ­vel via Swagger UI. Para acessÃ¡-la, inicie a aplicaÃ§Ã£o e entre em:
```
http://localhost:8080/swagger-ui/index.html
```

---

## ğŸ§ª Testando a API com Insomnia

Para facilitar os testes e o consumo da API, uma collection do Insomnia estÃ¡ disponÃ­vel no projeto. VocÃª pode importÃ¡-la diretamente no seu Insomnia a partir do seguinte arquivo:

- [**Collection Insomnia**](docs/collections/insomnia-collection.yaml)

---

## ğŸš€ Como Executar Localmente

Existem duas formas de executar o projeto localmente, dependendo do seu objetivo.

### OpÃ§Ã£o 1: Apenas a AplicaÃ§Ã£o (com Docker Compose)

Esta Ã© a forma mais rÃ¡pida e simples de subir a aplicaÃ§Ã£o e o banco de dados, ideal para desenvolvimento focado na API.

1.  Para executar, rode o comando na raiz do projeto:

    ```bash
    docker-compose up
    ```
2.  A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em `http://localhost:8080`.

3.  Para acessar a documentaÃ§Ã£o interativa e testar os endpoints, abra o Swagger UI no seu navegador:
    ```
    http://localhost:8080/swagger-ui/index.html
    ```

### OpÃ§Ã£o 2: Ambiente Kubernetes Completo (com Minikube)

Esta abordagem simula um ambiente Kubernetes real na sua mÃ¡quina, sendo ideal para testar os manifestos da pasta `infra/k8s` e a interaÃ§Ã£o completa dos serviÃ§os.

#### 1. PrÃ©-requisitos

- **Docker:** [InstruÃ§Ãµes de instalaÃ§Ã£o](https://docs.docker.com/engine/install/)
- **kubectl:** [InstruÃ§Ãµes de instalaÃ§Ã£o](https://kubernetes.io/docs/tasks/tools/install-kubectl/)
- **Minikube:** Siga o guia de instalaÃ§Ã£o oficial para o seu sistema operacional.
  - **Link Oficial:** https://minikube.sigs.k8s.io/docs/start/

#### 2. Inicie o Cluster Minikube

Abra seu terminal e execute o comando para iniciar o cluster:
```bash
minikube start
```

#### 3. Configure o Ambiente Docker

Este Ã© o passo mais importante. VocÃª precisa configurar seu terminal para usar o ambiente Docker de dentro do Minikube. Isso garante que a imagem que vocÃª construir estarÃ¡ visÃ­vel para o Kubernetes.
```bash
eval $(minikube -p minikube docker-env)
```
**Dica:** VocÃª precisarÃ¡ rodar este comando em cada novo terminal que abrir para interagir com o Minikube.

#### 4. Construa a Imagem da AplicaÃ§Ã£o

Com o ambiente configurado, navegue atÃ© a raiz do projeto e construa a imagem Docker.
```bash
docker build -t alexmarquesfa/oficina:latest .
```
**Importante:** O nome da imagem (`alexmarquesfa/oficina`) deve ser o mesmo que estÃ¡ definido no arquivo `app-deployment.yaml`. O uso da tag `:latest` faz com que o Kubernetes nÃ£o tente buscar a imagem de um repositÃ³rio remoto (`imagePullPolicy: IfNotPresent`).

#### 5. Aplique os Manifestos Kubernetes

Execute o script que aplica os manifestos na ordem correta, excluindo recursos especÃ­ficos da nuvem:
```bash
bash scripts/apply-local-k8s.sh
```
ApÃ³s alguns instantes, todos os recursos (Pods, Services, Deployments, etc.) estarÃ£o sendo criados.

#### 6. Acesse a AplicaÃ§Ã£o

Para acessar o serviÃ§o, que foi exposto como `NodePort`, use o seguinte comando do Minikube. Ele abrirÃ¡ a URL diretamente no seu navegador:
```bash
minikube service oficina-app-service -n oficina
```

#### 7. (Opcional) Monitoramento e Limpeza

- **Verificar status dos Pods:** `kubectl get pods -n oficina`
- **Parar o cluster:** `minikube stop`
- **Deletar o cluster:** `minikube delete`

---

## ğŸ—ï¸ Infraestrutura como CÃ³digo (Terraform)

![Desenho da Arquitetura EKS](docs/eks.png)

Toda a infraestrutura na AWS necessÃ¡ria para rodar esta aplicaÃ§Ã£o Ã© gerenciada como cÃ³digo usando o Terraform. Os arquivos de configuraÃ§Ã£o se encontram no diretÃ³rio `infra/terraform`.

A arquitetura provisionada inclui os seguintes recursos principais:

-   **VPC:** Uma Virtual Private Cloud Ã© criada usando o mÃ³dulo `terraform-aws-modules/vpc/aws` para isolar os recursos da aplicaÃ§Ã£o. Ela Ã© configurada com sub-redes pÃºblicas e privadas.
-   **NAT Gateway:** Um NAT Gateway Ã© habilitado para permitir que os recursos nas sub-redes privadas (como os nÃ³s do EKS) tenham acesso Ã  internet para baixar imagens e atualizaÃ§Ãµes, sem serem expostos diretamente.
-   **Cluster EKS:** O "cÃ©rebro" do Kubernetes Ã© provisionado usando o mÃ³dulo `terraform-aws-modules/eks/aws`. A configuraÃ§Ã£o inclui:
    -   Acesso pÃºblico ao API Server para permitir o deploy via GitHub Actions.
    -   Criptografia de segredos usando uma chave KMS existente.
    -   AutorizaÃ§Ã£o de acesso para o usuÃ¡rio do pipeline via EKS Access Entries.
-   **Node Group:** Um grupo de instÃ¢ncias EC2 (`t3.micro`) Ã© criado para servir como os nÃ³s de trabalho (workers) do cluster, onde os contÃªineres da aplicaÃ§Ã£o efetivamente rodam.
-   **Backend Remoto (S3 + DynamoDB):** O estado do Terraform Ã© gerenciado remotamente em um bucket S3, com travamento (locking) de estado via DynamoDB. Isso garante a seguranÃ§a e consistÃªncia ao trabalhar em equipe ou com pipelines de CI/CD.

---

## ğŸ”„ Fluxo de CI/CD (GitHub Actions)

O projeto utiliza GitHub Actions para automaÃ§Ã£o de integraÃ§Ã£o e deploy contÃ­nuo. Os workflows estÃ£o definidos em `.github/workflows`.

### CI - IntegraÃ§Ã£o ContÃ­nua (`ci-pipeline.yml`)

Este pipeline Ã© acionado a cada `push` em uma branch com o padrÃ£o `feature/*`. Seu objetivo Ã© garantir a qualidade e a integridade do cÃ³digo antes que ele seja mesclado Ã  `main`.

-   **Etapas:**
    1.  **Build e Testes:** Compila o cÃ³digo Java da aplicaÃ§Ã£o e executa todos os testes unitÃ¡rios.
    2.  **ValidaÃ§Ã£o Terraform:** Roda `terraform plan` para garantir que o cÃ³digo de infraestrutura estÃ¡ sintaticamente correto e Ã© aplicÃ¡vel.
    3.  **ValidaÃ§Ã£o Docker:** ConstrÃ³i a imagem Docker para garantir que o `Dockerfile` estÃ¡ funcionando.
    4.  **Criar Pull Request:** Se todas as etapas anteriores passarem, um Pull Request Ã© criado automaticamente para a branch `main`, sinalizando que a feature estÃ¡ pronta para revisÃ£o.

### CD - Deploy ContÃ­nuo (`cd-pipeline.yml`)

Este pipeline Ã© acionado automaticamente apÃ³s um `merge` na branch `main`. Seu objetivo Ã© colocar a nova versÃ£o da aplicaÃ§Ã£o em produÃ§Ã£o no ambiente da AWS.

-   **Etapas:**
    1.  **Publicar Imagem no Docker Hub:** ConstrÃ³i a imagem Docker da aplicaÃ§Ã£o, a identifica com uma tag Ãºnica (o hash do commit) e a envia para o Docker Hub.
    2.  **Deploy da Infraestrutura:** Roda `terraform apply` para aplicar qualquer mudanÃ§a pendente na infraestrutura do EKS ou da VPC.
    3.  **Deploy da AplicaÃ§Ã£o:** Executa os seguintes passos:
        -   Usa `kustomize` para atualizar o manifesto do `Deployment` com a tag da nova imagem Docker.
        -   Usa `kubectl apply` para aplicar os manifestos Kubernetes no cluster EKS.
        -   Verifica o status do rollout para garantir que a nova versÃ£o subiu com sucesso.

---

## ğŸ“‚ Estrutura do Projeto

O projeto Ã© modularizado por contexto de negÃ³cio. A partir do diretÃ³rio `src/main/java/br/com/alexsdm/postech/oficina/module`, cada mÃ³dulo representa um domÃ­nio especÃ­fico:

```text
cliente/
orcamento/
ordem_servico/
peca_insumo/
servico/
# ... e outros
```

### Estrutura Interna de cada MÃ³dulo (Ports and Adapters)


Cada mÃ³dulo segue a arquitetura hexagonal, organizada da seguinte forma:

```text
â”œâ”€â”€ core/                     # O nÃºcleo do domÃ­nio (coraÃ§Ã£o da aplicaÃ§Ã£o)
â”‚   â”œâ”€â”€ domain/               # ContÃ©m as entidades, VOs e regras de negÃ³cio puras
â”‚   â”œâ”€â”€ port/                 # Define as "portas" (interfaces) de comunicaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ in/               # Portas de entrada (o que a aplicaÃ§Ã£o oferece, ex: IAtualizarClienteUseCase)
â”‚   â”‚   â””â”€â”€ out/              # Portas de saÃ­da (o que a aplicaÃ§Ã£o precisa, ex: IClienteRepository)
â”‚   â””â”€â”€ usecase/              # ImplementaÃ§Ã£o das portas de entrada, orquestrando a lÃ³gica
â”‚
â””â”€â”€ adapters/                 # ImplementaÃ§Ãµes concretas das portas
    â”œâ”€â”€ in/                   # Adaptadores de entrada (driving adapters)
    â”‚   â””â”€â”€ controller/       # Ex: Controladores REST que recebem requisiÃ§Ãµes HTTP
    â””â”€â”€ out/                  # Adaptadores de saÃ­da (driven adapters)
        â””â”€â”€ persistence/      # Ex: ImplementaÃ§Ã£o do repositÃ³rio usando Spring Data JPA
```

-   **Core**: Ã‰ o centro da aplicaÃ§Ã£o, livre de dependÃªncias externas (frameworks, bancos de dados). ContÃ©m a lÃ³gica de negÃ³cio pura.
-   **Ports**: SÃ£o as interfaces que definem os contratos de comunicaÃ§Ã£o. As portas de entrada (`in`) sÃ£o implementadas pelos `usecases`, enquanto as portas de saÃ­da (`out`) sÃ£o implementadas pelos adaptadores de persistÃªncia ou clientes de outras APIs.
-   **Adapters**: SÃ£o a "ponte" entre o nÃºcleo e o mundo exterior. Eles adaptam as tecnologias especÃ­ficas (como HTTP, JPA, etc.) para as interfaces definidas nas portas.

Essa estrutura garante que o nÃºcleo da aplicaÃ§Ã£o permaneÃ§a isolado e testÃ¡vel, independentemente das tecnologias utilizadas na camada de infraestrutura.

---

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.